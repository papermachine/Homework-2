{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Naive Bayes\n",
    "## Due September 19th\n",
    "\n",
    "## Legal reasoning (From Murphy, 2.2). (25 pts)\n",
    "\n",
    "Suppose a crime has been committed. Blood is found at the scene for which there is no innocent explanation. It is of a type which is present in 1% of the population.  The defendant is known to have this rare blood type.  The  prosecutor claims: “There is a 1% chance that the defendant would have the crime blood type if he were innocent. Thus there is a 99% chance that he guilty”. This is known as the prosecutor’s fallacy. What is wrong with this argument?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright you low rate prosecutor, let's talk about this fallacy you just made. \n",
    "Lets assume that testing the blood on the knife is an infallable test and no false positives ever, it wont change this argument.\n",
    "- Its a true statement that 1% of the population DOES have this blood type. \n",
    "- Its true there is 1 murderer with this blood type out in the world somewhere, based on our assumed infallable test on the weapon.\n",
    "\n",
    "What you probably meant is that given my client were innocent, there would only be a 1% chance that my client would match the blood sample:\n",
    "$$P(\\mathrm{Blood Match}|\\mathrm{Innocence}) = \\frac{1}{100} $$\n",
    "\n",
    "But, what you claim __fallaciously__ is that;\n",
    "$$P(\\mathrm{Innocence}|\\mathrm{Blood Match}) = \\frac{1}{100} $$\n",
    "\n",
    "Yet, that couldn't be further from the truth. Let's call S the set of everyone with any blood type. 99% of that set S is innocent for certain. That leaves 1% of S that share my client's blood type. \n",
    "1% of S, how many people is that? We could live in a TINY town with a population of 10000. 1% of 10000 is 100 people. Let's talk about the probability of you picking the right person in this tiny town based on the statistics you brought to this trial. \n",
    "\n",
    "Prosecutor, the probability you have the right person is; $$ \\frac{1}{100} = 0.01 $$ in this tiny town.\n",
    "\n",
    "An the probability you are pursuing the wrong person is;\n",
    "$$ 1 - \\frac{1}{100} = 0.99 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham vs Spam (75 pts)\n",
    "One use of the naive Bayes classifier, which is still in practical use today, is as a spam filter.  Consider the corpus of text messages packaged with this homework, which are each labelled as either 'spam' or 'ham'.  In this case, naive Bayes utilizes a Bernoulli model that quantifies the probability of a given word given that the message is either spam or ham.  For example, investigating the text messages here, we find that the word *draw* shows up in spam 27 times, implying that\n",
    "$$P(X=\\mathrm{draw}|Y=\\mathrm{spam}) = \\frac{27}{25748} = \\frac{m_{draw,spam}}{m_{spam}},$$\n",
    "while in the case of ham, it shows up 5 times so\n",
    "$$P(X=\\mathrm{draw}|Y=\\mathrm{ham}) = \\frac{5}{67148} = \\frac{m_{draw,ham}}{m_{ham}}.$$\n",
    "Thus we see that the word 'draw' shows up with a much higher frequency in spam e-mails than in ham.\n",
    "\n",
    "While this is not particularly strong evidence on its own, we can create a powerful classifier by using the naive assumption in conjunction with all the words in a given message:\n",
    "$$ P(Y=\\mathrm{ham}|X=x) \\propto P(Y=\\mathrm{ham}) \\prod_{i=1}^n P(X_i=x_i|Y=\\mathrm{ham}), $$\n",
    "$$ P(Y=\\mathrm{spam}|X=x) \\propto P(Y=\\mathrm{spam}) \\prod_{i=1}^n P(X_i=x_i|Y=\\mathrm{spam}), $$\n",
    "where $x_i$ are the words in a given message. \n",
    "\n",
    "Your task is to write such a classifier.  I have taken the somewhat tedious step of parsing the data for you, yielding the variables *word_dictionary*, which contains the ham and spam counts for each word, as well as *training_labels*, which provides the spam/ham labels for each text message.  I have also parsed a set of test data: *test_messages* is a list, each entry containing another list of the words in the text message, as well as *test_labels* which contains the spam/ham label for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': [27, 189],\n",
       " \"i'm\": [6, 299],\n",
       " 'good': [12, 153],\n",
       " 'for': [157, 393],\n",
       " 'the': [164, 889],\n",
       " 'movie,': [1, 2],\n",
       " 'is': [124, 579],\n",
       " 'it': [21, 359],\n",
       " 'ok': [5, 127],\n",
       " 'if': [26, 287],\n",
       " 'i': [29, 1722],\n",
       " 'leave': [3, 41],\n",
       " 'in': [54, 629],\n",
       " 'an': [19, 68],\n",
       " 'hourish?': [1, 2],\n",
       " 'no:)this': [1, 2],\n",
       " 'kallis': [1, 4],\n",
       " 'home': [3, 92],\n",
       " 'ground.amla': [1, 2],\n",
       " 'town': [3, 19],\n",
       " 'durban:)': [1, 2],\n",
       " 'so': [21, 315],\n",
       " 'lets': [2, 14],\n",
       " 'make': [10, 73],\n",
       " 'saturday': [2, 6],\n",
       " 'or': [144, 180],\n",
       " 'monday': [1, 6],\n",
       " 'as': [28, 111],\n",
       " 'per': [36, 9],\n",
       " 'convenience.': [1, 2],\n",
       " 'hey...': [1, 5],\n",
       " 'what': [15, 188],\n",
       " 'time': [17, 121],\n",
       " 'your': [206, 327],\n",
       " 'driving': [2, 15],\n",
       " 'on': [106, 299],\n",
       " 'fri?': [1, 3],\n",
       " 'we': [36, 242],\n",
       " 'go': [20, 199],\n",
       " 'evaluation': [1, 2],\n",
       " '449050000301': [2, 1],\n",
       " 'you': [196, 1289],\n",
       " 'have': [104, 330],\n",
       " 'won': [49, 1],\n",
       " 'a': [302, 837],\n",
       " '£2,000': [11, 1],\n",
       " 'price!': [2, 1],\n",
       " 'to': [533, 1234],\n",
       " 'claim,': [3, 1],\n",
       " 'call': [268, 168],\n",
       " '09050000301.': [2, 1],\n",
       " 'going': [5, 125],\n",
       " '4': [76, 137],\n",
       " 'lunch': [1, 24],\n",
       " 'now': [76, 124],\n",
       " 'wif': [1, 25],\n",
       " 'my': [9, 582],\n",
       " 'family': [1, 14],\n",
       " 'then': [11, 153],\n",
       " 'aft': [1, 15],\n",
       " 'dat': [1, 28],\n",
       " 'str': [1, 3],\n",
       " '2': [129, 251],\n",
       " 'orchard': [1, 9],\n",
       " 'lor.': [1, 58],\n",
       " 'bored': [5, 10],\n",
       " 'of': [75, 410],\n",
       " 'speed': [2, 3],\n",
       " 'dating?': [2, 1],\n",
       " 'try': [5, 37],\n",
       " 'speedchat,': [2, 1],\n",
       " 'txt': [102, 11],\n",
       " 'speedchat': [2, 1],\n",
       " '80155,': [2, 1],\n",
       " \"don't\": [10, 105],\n",
       " 'like': [13, 172],\n",
       " 'em': [2, 8],\n",
       " 'swap': [2, 1],\n",
       " 'and': [93, 673],\n",
       " 'get': [68, 234],\n",
       " 'new': [57, 52],\n",
       " 'chatter!': [2, 1],\n",
       " 'chat80155': [2, 1],\n",
       " 'pobox36504w45wq': [5, 1],\n",
       " '150p/msg': [7, 1],\n",
       " 'rcd': [2, 1],\n",
       " '16': [18, 2],\n",
       " 'cancel': [6, 3],\n",
       " 'cheyyamo?and': [1, 2],\n",
       " 'some': [5, 92],\n",
       " 'money': [3, 30],\n",
       " 'back?': [2, 5],\n",
       " 'do': [20, 288],\n",
       " 'want': [23, 125],\n",
       " '750': [14, 1],\n",
       " 'anytime': [9, 2],\n",
       " 'any': [21, 87],\n",
       " 'network': [18, 2],\n",
       " 'mins': [20, 10],\n",
       " '150': [6, 1],\n",
       " 'text': [84, 49],\n",
       " 'video': [23, 3],\n",
       " 'phone': [22, 56],\n",
       " 'only': [54, 94],\n",
       " 'five': [3, 3],\n",
       " 'pounds': [13, 2],\n",
       " 'week': [25, 30],\n",
       " '08000776320': [3, 1],\n",
       " 'reply': [80, 18],\n",
       " 'delivery': [14, 1],\n",
       " 'tomorrow': [5, 45],\n",
       " 'ok.ok': [1, 4],\n",
       " 'ok..then..whats': [1, 4],\n",
       " 'ur': [103, 215],\n",
       " 'todays': [14, 4],\n",
       " 'plan': [1, 21],\n",
       " 'morning': [1, 37],\n",
       " 'princess!': [1, 16],\n",
       " 'how': [7, 205],\n",
       " 'are': [64, 313],\n",
       " 'you?': [1, 23],\n",
       " 'aiyar': [1, 5],\n",
       " 'sorry': [2, 64],\n",
       " 'lor': [1, 20],\n",
       " 'forgot': [1, 22],\n",
       " 'tell': [14, 100],\n",
       " 'u...': [1, 14],\n",
       " 'taking': [2, 15],\n",
       " 'part': [9, 17],\n",
       " 'our': [66, 44],\n",
       " 'mobile': [88, 15],\n",
       " 'survey': [2, 1],\n",
       " 'yesterday!': [2, 1],\n",
       " 'can': [26, 285],\n",
       " '500': [17, 1],\n",
       " 'texts': [14, 5],\n",
       " 'use': [8, 28],\n",
       " 'however': [2, 6],\n",
       " 'wish.': [3, 6],\n",
       " 'txts': [8, 1],\n",
       " 'just': [67, 225],\n",
       " 'send': [54, 92],\n",
       " '80160': [2, 1],\n",
       " 't&c': [12, 1],\n",
       " 'www.txt43.com': [2, 1],\n",
       " '1.50p': [2, 1],\n",
       " 'not': [21, 306],\n",
       " 'tonight': [1, 23],\n",
       " 'mate.': [1, 3],\n",
       " 'catching': [1, 4],\n",
       " 'up': [16, 208],\n",
       " 'sleep.': [1, 10],\n",
       " 'this': [75, 179],\n",
       " 'number': [24, 39],\n",
       " 'by': [29, 109],\n",
       " 'way.': [1, 10],\n",
       " 'height': [1, 4],\n",
       " '\"oh': [1, 5],\n",
       " 'shit....!!\"': [1, 2],\n",
       " 'situation:': [1, 2],\n",
       " 'guy': [1, 22],\n",
       " 'throws': [1, 2],\n",
       " 'luv': [5, 19],\n",
       " 'letter': [1, 3],\n",
       " 'gal': [1, 5],\n",
       " 'but': [8, 338],\n",
       " 'falls': [1, 3],\n",
       " 'her': [7, 74],\n",
       " 'brothers': [1, 2],\n",
       " 'head': [1, 15],\n",
       " 'whos': [1, 4],\n",
       " 'gay,.;-):-d': [1, 2],\n",
       " 'hmv': [8, 1],\n",
       " 'quiz': [7, 1],\n",
       " 'cash-balance': [6, 1],\n",
       " 'currently': [9, 4],\n",
       " '£500': [14, 1],\n",
       " '-': [45, 27],\n",
       " 'maximize': [6, 1],\n",
       " 'cash-in': [6, 1],\n",
       " 'hmv1': [2, 1],\n",
       " '86688': [16, 1],\n",
       " 'check': [3, 22],\n",
       " 'errors': [1, 2],\n",
       " 'had': [13, 69],\n",
       " 'difficulties,': [1, 2],\n",
       " 'correction.': [1, 2],\n",
       " 'howz': [1, 7],\n",
       " 'pain?hope': [1, 2],\n",
       " 'u': [93, 735],\n",
       " 'r': [18, 114],\n",
       " 'fine..': [1, 2],\n",
       " 'sorry,': [2, 43],\n",
       " \"i'll\": [1, 139],\n",
       " 'later': [1, 79],\n",
       " 'entered': [5, 4],\n",
       " 'cabin': [1, 4],\n",
       " 'pa': [1, 7],\n",
       " 'said,': [1, 7],\n",
       " \"''\": [1, 9],\n",
       " 'happy': [1, 75],\n",
       " \"b'day\": [1, 4],\n",
       " 'boss': [1, 6],\n",
       " \"!!''.\": [1, 4],\n",
       " 'felt': [1, 9],\n",
       " 'special.': [2, 4],\n",
       " 'she': [1, 109],\n",
       " 'askd': [1, 8],\n",
       " 'me': [19, 479],\n",
       " 'lunch.': [1, 7],\n",
       " 'after': [5, 68],\n",
       " 'invited': [5, 5],\n",
       " 'apartment.': [1, 4],\n",
       " 'went': [1, 52],\n",
       " 'there.': [1, 26],\n",
       " 'wake': [1, 23],\n",
       " 'already?': [1, 11],\n",
       " 'thanx': [1, 13],\n",
       " 'e': [2, 65],\n",
       " 'tau': [1, 2],\n",
       " 'sar': [1, 3],\n",
       " 'piah': [1, 2],\n",
       " \"it's\": [5, 71],\n",
       " 'quite': [1, 30],\n",
       " 'nice.': [1, 7],\n",
       " 'k': [1, 38],\n",
       " 'need': [7, 115],\n",
       " 'login': [2, 2],\n",
       " 'anything': [1, 53],\n",
       " 'dont': [9, 106],\n",
       " 'forget': [3, 16],\n",
       " 'place': [6, 29],\n",
       " 'many': [4, 48],\n",
       " 'free': [144, 42],\n",
       " 'requests': [2, 1],\n",
       " 'with': [85, 218],\n",
       " '1stchoice.co.uk': [2, 1],\n",
       " 'more': [22, 60],\n",
       " 'information': [7, 3],\n",
       " '08707808226.': [2, 1],\n",
       " 'lol': [1, 47],\n",
       " '...': [2, 133],\n",
       " 'was': [7, 183],\n",
       " 'busy': [3, 13],\n",
       " '*': [2, 34],\n",
       " 'wearing?': [1, 2],\n",
       " 'message:some': [1, 2],\n",
       " 'missing*': [1, 3],\n",
       " 'sender:name': [1, 2],\n",
       " '*number': [1, 2],\n",
       " 'missing': [2, 15],\n",
       " '*sent:date': [1, 2],\n",
       " '*missing': [1, 2],\n",
       " 'lot': [1, 21],\n",
       " 'thats': [1, 31],\n",
       " 'y': [1, 31],\n",
       " 'everything': [1, 20],\n",
       " 'sent': [7, 43],\n",
       " 'via': [3, 8],\n",
       " 'fullonsms.com': [1, 6],\n",
       " 'oh:)as': [1, 2],\n",
       " 'usual': [1, 6],\n",
       " 'vijay': [1, 4],\n",
       " 'film': [2, 8],\n",
       " 'its': [6, 168],\n",
       " 'different?': [1, 2],\n",
       " 'know': [17, 165],\n",
       " 'me.': [8, 75],\n",
       " 'chat': [35, 10],\n",
       " \"let's\": [5, 3],\n",
       " 'find': [16, 40],\n",
       " 'each': [8, 8],\n",
       " 'other!': [4, 1],\n",
       " 'rcvd.': [5, 1],\n",
       " 'hg/suite342/2lands/row/w1j6hl': [4, 1],\n",
       " 'ldn.': [4, 1],\n",
       " '18': [16, 1],\n",
       " 'years': [5, 14],\n",
       " 'over.': [4, 3],\n",
       " 'day?': [1, 8],\n",
       " 'mine': [1, 14],\n",
       " 'really': [1, 62],\n",
       " 'much': [2, 77],\n",
       " 'night?': [2, 7],\n",
       " 'there': [8, 102],\n",
       " 'way': [1, 60],\n",
       " \"shade's\": [1, 2],\n",
       " 'stuff': [1, 27],\n",
       " 'her.': [1, 12],\n",
       " 'has': [26, 65],\n",
       " 'been': [34, 63],\n",
       " 'wonderful': [1, 12],\n",
       " 'too.': [1, 14],\n",
       " 'really...': [1, 2],\n",
       " 'tot': [1, 20],\n",
       " 'paper': [1, 6],\n",
       " 'ended': [1, 5],\n",
       " 'long': [1, 27],\n",
       " 'ago...': [1, 2],\n",
       " 'wat': [2, 58],\n",
       " 'copied': [1, 2],\n",
       " 'jus': [1, 27],\n",
       " 'got': [8, 182],\n",
       " 'use?': [1, 2],\n",
       " 'lar...': [1, 14],\n",
       " 'still': [5, 114],\n",
       " 'haf': [1, 21],\n",
       " 'study': [1, 7],\n",
       " ':-(': [1, 16],\n",
       " 'thank': [2, 19],\n",
       " 'you,': [3, 24],\n",
       " 'winner': [9, 1],\n",
       " 'notified': [2, 1],\n",
       " 'sms.': [3, 2],\n",
       " 'luck!': [6, 2],\n",
       " 'future': [5, 2],\n",
       " 'marketing': [2, 1],\n",
       " 'stop': [70, 25],\n",
       " '84122': [2, 1],\n",
       " 'customer': [28, 8],\n",
       " 'services': [10, 2],\n",
       " '08450542832': [2, 1],\n",
       " 'babe': [7, 26],\n",
       " '?': [5, 149],\n",
       " 'lost': [3, 8],\n",
       " 'ok...': [1, 30],\n",
       " 'help': [7, 24],\n",
       " 'ask': [1, 68],\n",
       " \"she's\": [1, 17],\n",
       " 'working': [1, 19],\n",
       " 'tmr': [1, 14],\n",
       " 'not?': [1, 7],\n",
       " 'driving...': [1, 2],\n",
       " 'raining!': [1, 2],\n",
       " 'caught': [1, 3],\n",
       " 'at': [21, 301],\n",
       " 'mrt': [1, 8],\n",
       " 'station': [2, 4],\n",
       " 'drop': [1, 14],\n",
       " 'tank': [1, 2],\n",
       " '(that': [1, 2],\n",
       " 'said': [1, 52],\n",
       " 'him': [1, 77],\n",
       " 'one': [8, 123],\n",
       " 'time?)': [1, 2],\n",
       " 'change': [1, 13],\n",
       " 'also...': [1, 2],\n",
       " \"1000's\": [2, 1],\n",
       " 'girls': [5, 9],\n",
       " 'local': [4, 2],\n",
       " 'who': [31, 55],\n",
       " 'virgins': [2, 1],\n",
       " '&': [77, 12],\n",
       " 'ready': [6, 22],\n",
       " '4fil': [3, 1],\n",
       " 'every': [29, 28],\n",
       " 'sexual': [2, 1],\n",
       " 'need.': [2, 5],\n",
       " 'theirs?': [2, 1],\n",
       " 'cute': [2, 5],\n",
       " '69911(£1.50p.': [2, 1],\n",
       " 'm)': [2, 1],\n",
       " 'did': [2, 93],\n",
       " 'sitter': [1, 2],\n",
       " 'kaitlyn?': [1, 2],\n",
       " 'sick': [1, 5],\n",
       " 'slept': [1, 7],\n",
       " 'all': [22, 184],\n",
       " 'day': [8, 79],\n",
       " 'yesterday.': [1, 4],\n",
       " 'man,': [1, 5],\n",
       " 'accidentally': [1, 4],\n",
       " 'left': [2, 24],\n",
       " 'silent': [1, 6],\n",
       " 'last': [9, 59],\n",
       " 'night': [4, 63],\n",
       " \"didn't\": [1, 37],\n",
       " 'til': [1, 17],\n",
       " 'hey..': [1, 2],\n",
       " 'something': [1, 46],\n",
       " 'came': [2, 24],\n",
       " 'min..': [1, 2],\n",
       " 'think': [3, 94],\n",
       " 'wun': [1, 6],\n",
       " 'be': [40, 254],\n",
       " 'signing': [1, 2],\n",
       " 'tmr..': [1, 2],\n",
       " 'hee': [1, 3],\n",
       " \"he's\": [1, 25],\n",
       " 'adult': [5, 2],\n",
       " 'would': [5, 56],\n",
       " 'learn': [3, 5],\n",
       " 'from': [97, 129],\n",
       " 'experience.': [1, 6],\n",
       " \"there's\": [5, 19],\n",
       " 'real': [11, 17],\n",
       " 'danger.': [1, 2],\n",
       " 'peeps': [1, 2],\n",
       " 'using': [3, 7],\n",
       " 'drugs': [1, 10],\n",
       " 'they': [13, 89],\n",
       " 'comment': [1, 2],\n",
       " 'hey!': [1, 3],\n",
       " 'veggie': [1, 2],\n",
       " 'pizza...': [1, 2],\n",
       " ':/': [1, 7],\n",
       " 'yun': [1, 5],\n",
       " 'buying...': [1, 2],\n",
       " 'school': [1, 18],\n",
       " 'offer': [9, 5],\n",
       " '2000': [1, 2],\n",
       " 'plus': [8, 18],\n",
       " 'only...': [1, 2],\n",
       " 'sure': [1, 38],\n",
       " 'neighbors': [1, 2],\n",
       " 'didnt': [1, 24],\n",
       " 'pick': [3, 58],\n",
       " 'k.': [1, 8],\n",
       " 'will': [35, 266],\n",
       " 'again': [1, 24],\n",
       " 'entry': [21, 1],\n",
       " 'wkly': [9, 1],\n",
       " 'comp': [9, 2],\n",
       " 'win': [47, 9],\n",
       " 'fa': [3, 1],\n",
       " 'cup': [5, 3],\n",
       " 'final': [13, 2],\n",
       " 'tkts': [4, 1],\n",
       " '21st': [2, 2],\n",
       " 'may': [4, 30],\n",
       " '2005.': [3, 1],\n",
       " '87121': [3, 1],\n",
       " 'receive': [25, 3],\n",
       " 'question(std': [2, 1],\n",
       " \"rate)t&c's\": [2, 1],\n",
       " 'apply': [13, 3],\n",
       " \"08452810075over18's\": [2, 1],\n",
       " 'theory:': [1, 3],\n",
       " 'argument': [1, 5],\n",
       " 'wins': [2, 3],\n",
       " 'd': [2, 87],\n",
       " 'situation,': [1, 3],\n",
       " 'loses': [1, 3],\n",
       " 'person.': [1, 5],\n",
       " 'argue': [1, 4],\n",
       " 'friends': [1, 29],\n",
       " 'just..': [1, 3],\n",
       " '.': [8, 201],\n",
       " 'kick': [3, 5],\n",
       " 'them': [1, 56],\n",
       " '&amp;': [1, 72],\n",
       " 'say,': [1, 6],\n",
       " 'always': [1, 40],\n",
       " 'correct.!': [1, 3],\n",
       " 'well.': [1, 9],\n",
       " 'im': [6, 67],\n",
       " 'computerless.': [1, 2],\n",
       " 'oreo': [1, 2],\n",
       " 'truffles': [1, 2],\n",
       " 'haha': [1, 19],\n",
       " 'yeah': [1, 52],\n",
       " 'see': [15, 103],\n",
       " 'that': [19, 352],\n",
       " 'now,': [2, 14],\n",
       " 'sec': [1, 2],\n",
       " 'am': [11, 155],\n",
       " 'having': [1, 32],\n",
       " 'sir': [2, 8],\n",
       " 'hot': [12, 6],\n",
       " 'air': [1, 5],\n",
       " 'balloon!': [1, 2],\n",
       " 'bus.': [1, 4],\n",
       " 'come': [2, 181],\n",
       " 'soon': [1, 31],\n",
       " 'otherwise': [1, 10],\n",
       " 'msgs': [5, 4],\n",
       " 'pass.they': [1, 2],\n",
       " 'silently': [1, 3],\n",
       " 'say': [1, 59],\n",
       " 'thinking': [1, 14],\n",
       " 'right': [4, 57],\n",
       " 'also': [3, 50],\n",
       " 'making': [1, 20],\n",
       " 'least': [1, 12],\n",
       " 'moment.': [1, 2],\n",
       " 'gd': [1, 12],\n",
       " 'nt.swt': [1, 2],\n",
       " 'drms': [1, 2],\n",
       " '@shesil': [1, 2],\n",
       " 'yeah,': [1, 18],\n",
       " 'probably': [1, 29],\n",
       " 'swing': [1, 10],\n",
       " 'once': [1, 20],\n",
       " 'roommate': [1, 3],\n",
       " 'finishes': [1, 2],\n",
       " 'his': [2, 49],\n",
       " 'girl': [3, 15],\n",
       " 'takes': [5, 11],\n",
       " 'take': [14, 86],\n",
       " 'wrc': [3, 1],\n",
       " 'rally': [5, 1],\n",
       " 'oz?': [3, 1],\n",
       " 'lucozade': [3, 1],\n",
       " 'energy!': [3, 1],\n",
       " 'le': [3, 1],\n",
       " '61200': [3, 1],\n",
       " '(25p),': [3, 1],\n",
       " 'packs': [3, 1],\n",
       " 'lucozade.co.uk/wrc': [3, 1],\n",
       " 'itcould': [3, 1],\n",
       " 'u!': [7, 6],\n",
       " 'melody!': [1, 2],\n",
       " 'ü': [1, 135],\n",
       " 'dun': [1, 47],\n",
       " 'gf?': [1, 2],\n",
       " 'yay!': [1, 3],\n",
       " 'better': [1, 28],\n",
       " 'told': [1, 43],\n",
       " '5': [6, 23],\n",
       " 'other': [2, 45],\n",
       " 'either.': [1, 3],\n",
       " 'horrible': [1, 6],\n",
       " 'eat': [1, 27],\n",
       " 'macs': [1, 2],\n",
       " 'until': [5, 15],\n",
       " 'abt': [1, 22],\n",
       " 'already': [1, 33],\n",
       " 'rite...': [1, 9],\n",
       " 'reply.': [1, 6],\n",
       " 'thk': [1, 41],\n",
       " 'toot': [1, 3],\n",
       " 'than': [3, 30],\n",
       " 'b4': [6, 8],\n",
       " 'b': [10, 42],\n",
       " 'prepared.': [1, 2],\n",
       " 'shall': [1, 24],\n",
       " 'eat?': [1, 3],\n",
       " 'he': [1, 146],\n",
       " 'fantastic': [6, 4],\n",
       " 'chance,': [1, 3],\n",
       " 'bigger': [1, 3],\n",
       " 'life': [3, 33],\n",
       " 'lift': [1, 6],\n",
       " 'losing': [1, 4],\n",
       " 'live,': [1, 2],\n",
       " 'first': [6, 33],\n",
       " 'person': [1, 26],\n",
       " 'die': [1, 6],\n",
       " 'n': [10, 120],\n",
       " 'v': [2, 20],\n",
       " 'q?': [1, 2],\n",
       " 'nw': [1, 4],\n",
       " 'hme': [1, 2],\n",
       " 'da..': [1, 4],\n",
       " 'outside': [1, 12],\n",
       " 'islands,': [1, 2],\n",
       " 'towards': [1, 4],\n",
       " 'hard': [3, 9],\n",
       " 'rock': [2, 6],\n",
       " \"you'll\": [1, 11],\n",
       " 'run': [1, 17],\n",
       " 'into': [6, 26],\n",
       " 'class': [3, 17],\n",
       " 'class.': [1, 10],\n",
       " 'chennai': [1, 9],\n",
       " 'velachery:)': [1, 2],\n",
       " 'flippin': [1, 2],\n",
       " 'shit': [1, 23],\n",
       " 'yet?': [1, 5],\n",
       " 'give': [7, 69],\n",
       " 'sec,': [1, 2],\n",
       " 'breaking': [1, 2],\n",
       " '&lt;#&gt;': [1, 233],\n",
       " 'cstore': [1, 2],\n",
       " 'bad': [2, 24],\n",
       " 'avoid': [1, 2],\n",
       " 'this?': [1, 3],\n",
       " 'yo,': [1, 8],\n",
       " 'around?': [1, 7],\n",
       " 'car': [1, 21],\n",
       " 'back': [17, 87],\n",
       " 'annoying': [1, 3],\n",
       " \"isn't\": [1, 8],\n",
       " 'it.': [2, 49],\n",
       " 'goodmorning,': [1, 6],\n",
       " 'today': [6, 54],\n",
       " 'late': [2, 31],\n",
       " 'min.': [5, 5],\n",
       " 'point': [1, 9],\n",
       " 'hangin': [1, 2],\n",
       " 'mr': [1, 7],\n",
       " 'makin': [1, 3],\n",
       " 'alive.better': [1, 2],\n",
       " 'correct': [3, 4],\n",
       " 'looking': [7, 15],\n",
       " 'figure': [1, 6],\n",
       " 'itself..': [1, 3],\n",
       " 'case': [1, 9],\n",
       " 'guess': [6, 18],\n",
       " 'campus': [1, 3],\n",
       " 'lodge': [1, 2],\n",
       " \"we're\": [1, 18],\n",
       " 'done...': [1, 4],\n",
       " 'wont': [1, 24],\n",
       " 'anything.': [1, 7],\n",
       " 'trust': [1, 6],\n",
       " 'too': [2, 56],\n",
       " 'worrying': [1, 2],\n",
       " 'about': [3, 119],\n",
       " 'appt.': [1, 2],\n",
       " 'shame': [1, 4],\n",
       " 'missed': [3, 19],\n",
       " 'out': [32, 136],\n",
       " 'quizzes': [1, 2],\n",
       " 'popcorn': [1, 2],\n",
       " 'doing': [1, 59],\n",
       " 'hair.': [1, 5],\n",
       " 'sex': [11, 3],\n",
       " 'sexy': [12, 11],\n",
       " 'pic': [9, 4],\n",
       " 'jordan!': [2, 1],\n",
       " '88600.': [2, 1],\n",
       " 'wk': [9, 5],\n",
       " 'celeb!': [2, 1],\n",
       " 'pocketbabe.co.uk': [4, 1],\n",
       " 'pics.': [2, 2],\n",
       " '£3/wk': [5, 1],\n",
       " '087016248': [2, 1],\n",
       " 'c': [16, 46],\n",
       " 'ya...': [1, 3],\n",
       " '1': [27, 24],\n",
       " 'voicemail.': [4, 1],\n",
       " 'please': [43, 58],\n",
       " '08719181503': [2, 1],\n",
       " 'matter.': [1, 2],\n",
       " 'mind': [1, 23],\n",
       " 'saying': [1, 18],\n",
       " 'matter': [1, 2],\n",
       " 'knows': [2, 6],\n",
       " 'menu': [1, 3],\n",
       " 'da.': [1, 32],\n",
       " 'al': [1, 6],\n",
       " 'does': [3, 15],\n",
       " 'moan': [1, 5],\n",
       " 'thin': [1, 2],\n",
       " 'goes': [1, 23],\n",
       " 'wrong': [1, 12],\n",
       " 'fault&al': [1, 2],\n",
       " 'de': [1, 13],\n",
       " 'arguments': [1, 2],\n",
       " 'fault&fed': [1, 2],\n",
       " 'himso': [1, 2],\n",
       " 'bother?': [1, 2],\n",
       " 'hav': [1, 22],\n",
       " '2go,': [1, 2],\n",
       " 'thanx.xx': [1, 2],\n",
       " 'neft': [1, 3],\n",
       " 'transaction': [1, 4],\n",
       " 'reference': [4, 5],\n",
       " 'rs.': [1, 4],\n",
       " '&lt;decimal&gt;': [1, 18],\n",
       " 'credited': [2, 4],\n",
       " 'beneficiary': [1, 3],\n",
       " 'account': [15, 10],\n",
       " '&lt;time&gt;': [1, 7],\n",
       " ':': [1, 10],\n",
       " 'job': [1, 22],\n",
       " 'na-tuition..': [1, 4],\n",
       " 'called': [2, 27],\n",
       " 'da,': [1, 3],\n",
       " 'feel': [1, 46],\n",
       " 'yesterday': [3, 9],\n",
       " 'wait': [1, 38],\n",
       " '2day': [5, 3],\n",
       " 'dear.': [1, 14],\n",
       " 'thanks': [13, 40],\n",
       " 'understanding.': [1, 2],\n",
       " \"i've\": [2, 47],\n",
       " 'trying': [11, 21],\n",
       " 'sura': [1, 3],\n",
       " 'that.': [1, 15],\n",
       " 'year': [4, 24],\n",
       " 'supply': [3, 1],\n",
       " 'cds': [5, 1],\n",
       " 'store': [8, 2],\n",
       " 'choice': [3, 2],\n",
       " 'worth': [10, 3],\n",
       " 'enter': [10, 4],\n",
       " '£100': [19, 1],\n",
       " 'weekly': [20, 1],\n",
       " 'draw': [28, 5],\n",
       " 'music': [13, 2],\n",
       " '87066': [11, 1],\n",
       " 'ts&cs': [8, 1],\n",
       " 'www.ldew.com.subs16+1win150ppmx3': [3, 1],\n",
       " 'whole': [1, 8],\n",
       " 'appreciated': [1, 2],\n",
       " 'two!': [1, 2],\n",
       " 'dad': [1, 25],\n",
       " 'map': [1, 3],\n",
       " 'reading': [1, 10],\n",
       " 'semi': [1, 2],\n",
       " 'apart': [1, 3],\n",
       " 'things': [3, 30],\n",
       " 'ok.': [1, 36],\n",
       " 'p.': [1, 2],\n",
       " 'sim': [2, 6],\n",
       " 'subscriber,': [2, 1],\n",
       " 'selected': [20, 1],\n",
       " 'bonus!': [2, 1],\n",
       " 'delivered': [5, 1],\n",
       " 'door,': [2, 2],\n",
       " 'word': [10, 10],\n",
       " 'no:': [14, 1],\n",
       " '88600': [2, 1],\n",
       " 'claim.': [4, 1],\n",
       " '150p/msg,': [2, 1],\n",
       " 'exp.': [2, 1],\n",
       " '30apr': [2, 1],\n",
       " 'strong': [1, 4],\n",
       " 'arms...': [1, 2],\n",
       " 'maaaan': [1, 2],\n",
       " 'bday': [1, 4],\n",
       " 'april': [1, 4],\n",
       " 'guessin': [1, 2],\n",
       " \"ain't\": [1, 3],\n",
       " 'gonna': [1, 48],\n",
       " 'here': [4, 63],\n",
       " 'before': [5, 42],\n",
       " '9?': [1, 3],\n",
       " 'half': [10, 21],\n",
       " 'hour': [1, 20],\n",
       " 'game': [5, 7],\n",
       " 'almost': [1, 11],\n",
       " 'over?': [2, 6],\n",
       " 'walmart': [1, 2],\n",
       " 'sure.': [1, 6],\n",
       " 'ilol': [1, 2],\n",
       " 'let': [1, 59],\n",
       " 'know,': [1, 5],\n",
       " 'personally': [1, 2],\n",
       " 'wuldnt': [1, 2],\n",
       " 'bother,': [1, 2],\n",
       " 'goin': [1, 18],\n",
       " 'mite': [1, 6],\n",
       " 'well!!': [1, 2],\n",
       " 'now!': [56, 3],\n",
       " 'creepy': [1, 3],\n",
       " \"won't\": [4, 13],\n",
       " '…': [1, 11],\n",
       " 'tomo': [1, 8],\n",
       " 'lunchtime,': [1, 2],\n",
       " 'i,': [1, 3],\n",
       " 'organise': [1, 2],\n",
       " 'something?': [1, 5],\n",
       " '08719181513.': [3, 1],\n",
       " 'damn,': [1, 3],\n",
       " \"k..k..i'm\": [1, 3],\n",
       " 'fine:)when': [1, 3],\n",
       " 'complete': [2, 7],\n",
       " 'course?': [1, 3],\n",
       " 'true.': [3, 4],\n",
       " 'passable.': [1, 2],\n",
       " 'high': [3, 2],\n",
       " 'score': [1, 4],\n",
       " 'phd,': [1, 2],\n",
       " '5years': [1, 2],\n",
       " 'salary.': [1, 2],\n",
       " 'makes': [1, 20],\n",
       " 'easier.': [1, 2],\n",
       " 'no.': [6, 26],\n",
       " 'nokia': [53, 4],\n",
       " 'tone': [37, 1],\n",
       " 'mob': [16, 1],\n",
       " 'week!': [11, 2],\n",
       " 'nok': [3, 1],\n",
       " '87021.': [4, 1],\n",
       " '1st': [23, 10],\n",
       " '!': [7, 47],\n",
       " 'txtin': [5, 1],\n",
       " 'friends.': [5, 4],\n",
       " '150p/tone.': [5, 1],\n",
       " 'hl': [6, 1],\n",
       " '4info': [5, 1],\n",
       " 'prakesh': [1, 2],\n",
       " 'know.': [1, 12],\n",
       " 'teach': [1, 6],\n",
       " 'apps': [1, 4],\n",
       " 'when': [12, 209],\n",
       " 'college.': [1, 3],\n",
       " 'rofl': [1, 3],\n",
       " 'betta': [1, 2],\n",
       " 'invest': [1, 3],\n",
       " 'anti': [1, 4],\n",
       " 'aging': [1, 2],\n",
       " 'products': [1, 2],\n",
       " 'specially': [7, 1],\n",
       " '£1000': [23, 1],\n",
       " 'cash': [43, 6],\n",
       " '4*': [20, 1],\n",
       " 'holiday': [26, 10],\n",
       " '(flights': [3, 1],\n",
       " 'inc)': [3, 1],\n",
       " 'speak': [9, 18],\n",
       " 'live': [23, 8],\n",
       " 'operator': [13, 1],\n",
       " 'claim': [78, 1],\n",
       " '0871277810810': [2, 1],\n",
       " 'sir,': [1, 12],\n",
       " 'another': [3, 25],\n",
       " '1hr': [1, 2],\n",
       " 'time.': [4, 18],\n",
       " 'delay.': [1, 3],\n",
       " 'name': [8, 23],\n",
       " 'address': [4, 7],\n",
       " 'post': [3, 14],\n",
       " 'weeks': [7, 4],\n",
       " 'completely': [2, 7],\n",
       " 'accommodation': [2, 1],\n",
       " 'various': [2, 3],\n",
       " 'global': [2, 1],\n",
       " 'locations': [4, 1],\n",
       " 'www.phb1.com': [2, 1],\n",
       " 'ph:08700435505150p': [2, 1],\n",
       " \"ü'll\": [1, 3],\n",
       " 'submitting': [1, 2],\n",
       " 'da': [1, 76],\n",
       " 'project': [1, 10],\n",
       " 'rite?': [1, 6],\n",
       " '£250': [13, 1],\n",
       " '84128': [3, 1],\n",
       " 'now.': [20, 60],\n",
       " 'www.textcomp.com': [3, 1],\n",
       " 'cust': [4, 1],\n",
       " 'care': [4, 38],\n",
       " '08712405020.': [4, 1],\n",
       " 'ans': [5, 4],\n",
       " 'lar.': [1, 8],\n",
       " \"u'll\": [1, 6],\n",
       " 'noe': [1, 13],\n",
       " 'later.': [1, 23],\n",
       " 'sell': [1, 6],\n",
       " 'fast.': [1, 2],\n",
       " 'easy': [3, 10],\n",
       " 'money.': [1, 8],\n",
       " 'few': [1, 31],\n",
       " 'do.': [1, 18],\n",
       " 'pub': [3, 10],\n",
       " \"1's\": [1, 3],\n",
       " 'finish': [1, 26],\n",
       " 'meeting': [1, 32],\n",
       " 'snatch': [1, 2],\n",
       " 'purse': [1, 3],\n",
       " '\"hello-/@drivby-:0quit': [1, 2],\n",
       " 'edrunk': [1, 2],\n",
       " 'iff': [1, 2],\n",
       " 'pthis': [1, 2],\n",
       " 'senrd-dnot': [1, 2],\n",
       " '^': [1, 2],\n",
       " 'dancce': [1, 2],\n",
       " 'drum': [1, 2],\n",
       " 'basq!ihave': [1, 2],\n",
       " 'fun': [7, 15],\n",
       " '2nhite': [1, 2],\n",
       " 'x': [7, 25],\n",
       " 'ros': [1, 2],\n",
       " 'xxxxxxx\"': [1, 2],\n",
       " 'opinion': [1, 6],\n",
       " 'me?': [2, 18],\n",
       " '1.': [2, 7],\n",
       " 'over': [6, 32],\n",
       " '2.': [2, 8],\n",
       " 'jada': [1, 3],\n",
       " '3.': [3, 7],\n",
       " 'kusruthi': [1, 3],\n",
       " '4.': [3, 8],\n",
       " 'lovable': [1, 6],\n",
       " '5.': [2, 4],\n",
       " '6.': [1, 4],\n",
       " 'spl': [1, 3],\n",
       " 'character': [1, 4],\n",
       " '7.': [1, 5],\n",
       " 'matured': [1, 3],\n",
       " '8.': [1, 3],\n",
       " 'stylish': [1, 3],\n",
       " '9.': [1, 5],\n",
       " 'simple': [2, 6],\n",
       " 'pls': [10, 73],\n",
       " 'reply..': [1, 3],\n",
       " 'getting?': [1, 2],\n",
       " 'morn': [1, 4],\n",
       " 'tmr?': [1, 9],\n",
       " 'dear': [11, 53],\n",
       " 'relieved': [1, 2],\n",
       " 'westonzoyland,': [1, 2],\n",
       " 'end': [7, 17],\n",
       " 'too!': [1, 4],\n",
       " 'hope': [5, 77],\n",
       " 'great': [11, 55],\n",
       " 'semester.': [1, 5],\n",
       " 'wish': [1, 40],\n",
       " 'very': [3, 61],\n",
       " 'best.': [5, 2],\n",
       " 'made': [2, 24],\n",
       " 'greatness.': [1, 2],\n",
       " 'oh': [2, 63],\n",
       " 'yes': [8, 31],\n",
       " 'no!': [1, 5],\n",
       " 'hmm.': [1, 3],\n",
       " 'email?': [1, 3],\n",
       " 'show': [5, 16],\n",
       " 'world,': [1, 2],\n",
       " 'princess': [1, 3],\n",
       " ':)': [1, 48],\n",
       " 'europe?': [1, 2],\n",
       " 'nobody': [1, 10],\n",
       " 'decide': [1, 8],\n",
       " 'where': [3, 78],\n",
       " 'wants': [2, 20],\n",
       " 'chinese': [1, 5],\n",
       " 'shoot': [1, 4],\n",
       " 'docs': [1, 4],\n",
       " 'waiting': [12, 27],\n",
       " 'room.': [1, 7],\n",
       " 'now?': [2, 23],\n",
       " 'dinner': [1, 21],\n",
       " 'soon..': [1, 2],\n",
       " 'hello': [3, 12],\n",
       " 'which': [7, 41],\n",
       " 'site': [1, 4],\n",
       " 'download': [3, 8],\n",
       " 'songs': [1, 5],\n",
       " 'urgent': [13, 2],\n",
       " 'mean,': [1, 2],\n",
       " 'king': [4, 4],\n",
       " 'havin': [2, 3],\n",
       " 'credit!': [2, 2],\n",
       " 'goin2bed': [1, 2],\n",
       " 'sweet!': [1, 3],\n",
       " 'only1more': [1, 2],\n",
       " 'sleep!': [1, 3],\n",
       " 'gal.': [1, 6],\n",
       " 'sch': [1, 15],\n",
       " 'stuff.': [1, 5],\n",
       " 'mc?': [1, 2],\n",
       " 'hi': [11, 72],\n",
       " 'hun!': [1, 3],\n",
       " 'comin': [1, 8],\n",
       " '2nite-tell': [1, 2],\n",
       " 'every1': [1, 2],\n",
       " 'me,': [1, 13],\n",
       " 'ava': [1, 2],\n",
       " 'goodtime!oli': [1, 2],\n",
       " 'rang': [1, 4],\n",
       " 'melnite': [1, 2],\n",
       " 'ifink': [1, 2],\n",
       " 'sorted,but': [1, 2],\n",
       " 'il': [1, 7],\n",
       " 'explain': [1, 4],\n",
       " 'everythin': [1, 2],\n",
       " 'mon.l8rs.x': [1, 2],\n",
       " 'later,': [1, 2],\n",
       " 'network.': [3, 2],\n",
       " 'urgnt,': [1, 2],\n",
       " 'sms': [15, 16],\n",
       " 'ummmmmaah': [1, 3],\n",
       " 'returns': [1, 5],\n",
       " 'sweet': [1, 22],\n",
       " 'heart..': [1, 3],\n",
       " 'birthday': [1, 22],\n",
       " '08712402779': [2, 1],\n",
       " 'immediately': [5, 1],\n",
       " 'message': [19, 33],\n",
       " 'imma': [1, 6],\n",
       " 'flip': [1, 3],\n",
       " 'mum': [1, 11],\n",
       " 'wan': [3, 48],\n",
       " 'go...': [1, 3],\n",
       " 'shun': [1, 2],\n",
       " 'bian': [1, 2],\n",
       " 'watch': [1, 26],\n",
       " 'glass': [1, 2],\n",
       " 'exhibition...': [1, 2],\n",
       " 'pongal?': [1, 2],\n",
       " 'till': [3, 19],\n",
       " 'march': [1, 11],\n",
       " 'el': [1, 2],\n",
       " 'nino': [1, 2],\n",
       " 'gets': [1, 9],\n",
       " 'himself.': [1, 2],\n",
       " 'oh.': [1, 19],\n",
       " 'yet': [2, 19],\n",
       " 'chikku..going': [1, 2],\n",
       " 'room': [1, 14],\n",
       " 'nw,': [1, 3],\n",
       " 'bus..': [1, 2],\n",
       " 'cbe': [1, 5],\n",
       " 'only.': [5, 8],\n",
       " 'pay.': [1, 2],\n",
       " 'honey': [1, 6],\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Maps from 'ham' or 'spam' strings to zero or one\n",
    "def mapper(s):\n",
    "    if s=='spam':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Read in the text file\n",
    "f = open('SMSSpamCollection','r')\n",
    "lines = f.readlines()\n",
    "\n",
    "# Break out the test data\n",
    "test_lines = lines[:len(lines)//5]\n",
    "lines = lines[len(lines)//5:]\n",
    "\n",
    "# Instantiate the frequency dictionary and an array to\n",
    "# record whether the line is ham or spam\n",
    "word_dictionary = {}\n",
    "training_labels = np.zeros(len(lines),dtype=int)\n",
    "\n",
    "# Loop over all the training messages\n",
    "for i,l in enumerate(lines):\n",
    "    # Split into words\n",
    "    l = l.lower().split()\n",
    "    # Record the special first word which always ham or spam\n",
    "    if l[0]=='ham':\n",
    "        training_labels[i] = 1\n",
    "   \n",
    "    # For each word in the message, record whether the message was ham or spam\n",
    "    for w in l[1:]:\n",
    "        # If we've never seen the word before, add a new dictionary entry\n",
    "        if w not in word_dictionary:\n",
    "            word_dictionary[w] = [1,1]\n",
    "        # If spam [word][+1, 0], if ham [word][0,+1]\n",
    "        word_dictionary[w][mapper(l[0])] += 1\n",
    "        \n",
    "# Loop over the test messages\n",
    "test_labels = np.zeros(len(test_lines),dtype=int)\n",
    "test_messages = []\n",
    "for i,l in enumerate(test_lines):\n",
    "    l = l.lower().split()\n",
    "    if l[0]=='ham':\n",
    "        test_labels[i] = 1\n",
    "    test_messages.append(l)\n",
    "\n",
    "counts = np.array([v for v in word_dictionary.values()]).sum(axis=0)\n",
    "\n",
    "# Scratch stuff\n",
    "word_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I have provided code skeletons. Your job is to make the code skeletons into an operational naive Bayes spam detector. (you may discard these skeletons if you would prefer to code this from scratch). Note that lines where you will need to change the code are marked with a '#!'.\n",
    "\n",
    "Your first task is train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868013468013468"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is the prior P(Y=ham) ?\n",
    "\n",
    "# ham messages / total messages = P(Y=ham)\n",
    "ham_prior = sum(training_labels)/len(word_dictionary)\n",
    "# Messages which are not ham are spam; the compliment ham\n",
    "spam_prior = 1 - ham_prior\n",
    "\n",
    "ham_prior \n",
    "spam_prior\n",
    "\n",
    "def w_tot(val):\n",
    "    return val[0] + val[1]\n",
    "\n",
    "# What are the class probabilities P(X=word|Y=ham) for each word?\n",
    "\n",
    "# P(X='word' | Y=spam) = (count 'word' found in spam)/(total word uses)\n",
    "# P(X='word' | Y=ham) = (count of 'word' found in ham)/(total word uses) \n",
    "\n",
    "ham_likelihood = {}\n",
    "spam_likelihood = {}\n",
    "for key,val in word_dictionary.items():\n",
    "    ham_likelihood[key] =  val[1] / w_tot(val)\n",
    "    spam_likelihood[key] = val[0] / w_tot(val)\n",
    "    \n",
    "ham_likelihood['you']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to make predictions on a set of test examples which were held back from the training procedure (see *test_messages* variable).  For each of these messages, compute the ham and spam probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where to hold the ham and spam posteriors\n",
    "posteriors = np.zeros((len(test_lines),2))\n",
    "\n",
    "# Loop over all the messages in the test set\n",
    "for i,m in enumerate(test_messages):\n",
    "    posterior_ham = 1.0\n",
    "    posterior_spam = 1.0\n",
    "    # Prior is multiplied into the posterior once\n",
    "    posterior_ham *= ham_prior\n",
    "    posterior_spam *= spam_prior\n",
    "    \n",
    "    # Loop over all the words in each message\n",
    "    for w in m:\n",
    "        # The purpose of this try/except handler is \n",
    "        # if we find a word which is not contained in the training data\n",
    "        # we have to throw it out as it has no prior probability \n",
    "        # otherwise we have a bad probability for the new word. \n",
    "        try:\n",
    "            posterior_ham *= 1 * ham_likelihood[w]\n",
    "            posterior_spam *= 1 * spam_likelihood[w]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    # Notice the normalization factor (denominator) \n",
    "    # to turn these into proper probabilities!\n",
    "    \n",
    "    \n",
    "    posteriors[i,0] = posterior_spam/(posterior_spam + posterior_ham)\n",
    "    posteriors[i,1] = posterior_ham/(posterior_spam + posterior_ham)\n",
    "    \n",
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make a ham/spam prediction based on your posterior probabilities.  Compare these to the labels contained in test_labels.  Report the accuracy of your classifier as percentage correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8456014362657092"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_correct = 0\n",
    "total_texts = len(posteriors)\n",
    "\n",
    "len(test_labels)\n",
    "\n",
    "for i,tlabel in enumerate(test_labels):\n",
    "    # tlabel = 1 if the test label was marked 'ham'\n",
    "    if (tlabel == 1) and (posteriors[i,0] < 0.5):\n",
    "        texts_correct += 1 \n",
    "\n",
    "classifier_accuracy = texts_correct / total_texts\n",
    "classifier_accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
